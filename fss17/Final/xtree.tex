\begin{figure}[htbp!]
\footnotesize
\centering
\resizebox{.95\linewidth}{!}{
\begin{tabular}{|p{0.95\linewidth}|} \hline
\begin{minipage}{\linewidth}
\small

{\bf \fig{xtree}.A: Top-down division with Decision Trees}

\be
\item Find a split in the values of independent features (OO metrics) that most 
reduces the variability
of the dependent feature (defect counts). For continuous and discrete values,
the {\em variability} can be measured using standard deviation $\sigma$ or 
entropy $e$ respectively. Construct a standard decision tree using these splits.

\item Discretize all numeric features using the Fayyad-Iranni 
discretizer~\cite{fi}
(divide numeric columns into bins $B_i$, each of which  select for the fewest 
cluster ids).
Let feature $F$ have bins $B_i$, each of which contains $n_i$ rows
and let each bin $B_i$ have entropy $e_i$ computed from the frequency of 
clusters seen in that bin.
Cull the the features as per~\cite{papa13}; i.e. just use the $\beta=33\%$ most 
informative features
where  the   value of  feature $F$ is $\sum_i e_i\frac{n_i}{N}$ ($N$ is the 
number of rows).\\[-0.1cm]
\ee
\end{minipage}
\\\hline\textbf{\fig{xtree}.B: A sample XTREE tree.}\\
\begin{minipage}{\linewidth}
% ~\hrule~
\centering
\includegraphics[width=\linewidth]{XTREE_samp.eps}
% ~\hrule~
\end{minipage}\bigstrut\\\hline
\\[-0.2cm]
\begin{minipage}{\linewidth}
\small
% \begin{shaded}  	   

{\small
{\bf \fig{xtree}.C: Using XTREE}

Using the training data,  divide the data using the decision tree algorithm of 
\fig{xtree}.A into groups of
size $\alpha=\sqrt{N}$.
For test item, 
	  find the {\em current } leaf: take each test instance, run it down to a leaf 
	  in the decision tree.  
After that,	  find the {\em desired} leaf:
		\begin{itemize}[leftmargin=3mm]
		\item Starting at {\em current}, ascend the tree $lvl\in \{0,1,2...\}$ levels;
		\item Identify {\em sibling} leaves; i.e. leaf clusters that can be reached 
		from level $lvl$ that are not same as {\em current }
		\item Using the {\em score} defined above, find the {\em better} siblings; 
		i.e. those with a {\em score} less than $\gamma=0.5$ times the mean score of 
		{\em current}. 
		   If none found, then repeat for $lvl += 1$. Also,
		    return no plan if the new $lvl$ is above the root. 
		\item  Return the {\em closest} better sibling where distance is measured 
		between the mean centroids of that sibling and {\em current}
		\end{itemize}
	 Also, find the {\em delta}; i.e. the set difference between  conditions in 
	 the decision tree branch to {\em desired} and {\em current}. To find that 
	 delta: (1)~for discrete attributes, delta is the value from {\em desired}; 
	 (2)~for  numerics, delta is the numeric difference; (3)~for numerics  
	 discretized into ranges, delta is a random number selected from the low and 
	 high boundaries of the that range.
	 
		Finally, return the delta as the plan for improving the test instance.}
% \end{shaded}
\end{minipage}\\\hline
\end{tabular}}
\caption{Generating thresholds using XTREE.}\label{fig:xtree}
\end{figure}